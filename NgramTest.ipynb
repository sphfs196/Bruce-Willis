{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held: \n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old, \n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"當地警察已在28日下午開始監控人潮，但遊客數量似乎不如預期的多，因為當局目前認定還沒有關閉閘門的必要。威尼斯絡繹不絕的觀光客，已經成為當地人的惡夢。當地巷弄窄小，在旅遊旺季街道幾乎難以行走。\"\n",
    "test_sentence = str(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = [ ([test_sentence[i], test_sentence[i+1]], test_sentence[i+2])\n",
    "            for i in range(len(test_sentence) - 2) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['當', '地'], '警'), (['地', '警'], '察'), (['警', '察'], '已'), (['察', '已'], '在'), (['已', '在'], '2'), (['在', '2'], '8')]\n"
     ]
    }
   ],
   "source": [
    "print(trigrams[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word : i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "print(len(vocab))#每個詞向量長度 = 97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2 \n",
    "EMBEDDING_DIM = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embdeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embdeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs, self.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.0881)\n",
      "tensor(6.8002)\n",
      "tensor(6.5768)\n",
      "tensor(6.3990)\n",
      "tensor(6.2546)\n",
      "tensor(6.1353)\n",
      "tensor(6.0352)\n",
      "tensor(5.9502)\n",
      "tensor(5.8771)\n",
      "tensor(5.8137)\n",
      "tensor(5.7582)\n",
      "tensor(5.7094)\n",
      "tensor(5.6660)\n",
      "tensor(5.6271)\n",
      "tensor(5.5922)\n",
      "tensor(5.5607)\n",
      "tensor(5.5321)\n",
      "tensor(5.5060)\n",
      "tensor(5.4821)\n",
      "tensor(5.4602)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2000):\n",
    "    total_loss = torch.Tensor([0])# total_loss = 0\n",
    "    \n",
    "    for context, target in trigrams:\n",
    "        # step 1:\n",
    "        # 將context(ex : ['When','forty'])轉換成index(ex : [68, 15])\n",
    "        # 再轉成pytorch的variable\n",
    "        context_idxs = [word_to_ix[w] for w in context]\n",
    "        context_var = autograd.Variable(torch.LongTensor(context_idxs))\n",
    "        \n",
    "        # step 2:\n",
    "        # 清空gradient，防止上一次的累計\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # step 3:\n",
    "        # 丟variable變數進去跑forward\n",
    "        log_probs, embedd = model(context_var)\n",
    "        \n",
    "        # step 4:\n",
    "        # 計算loss(把target variable丟進去)\n",
    "        loss = loss_function(log_probs, autograd.Variable(torch.LongTensor([word_to_ix[target]])))\n",
    "        \n",
    "        # step 5:\n",
    "        # 跑backward，更新gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data\n",
    "    losses.append(total_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        print(total_loss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input context =  ['地', '警']\n",
      "tensor([ 59])\n",
      "real word is 察, predict word is 察\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miplab01\\Anaconda3\\envs\\jiebaTest\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "word, target_word = trigrams[1]\n",
    "print('input context = ', word)\n",
    "word = autograd.Variable(torch.LongTensor([word_to_ix[i] for i in word]))\n",
    "out, outEmbedd = model(word)\n",
    "# torch.max : Returns the maximum value of each row of the input tensor \n",
    "# in the given dimension dim(第二個參數). \n",
    "# The second return value is the index location of each maximum value found.\n",
    "_, predict_label = torch.max(out,1)\n",
    "print(predict_label)\n",
    "predict_word = ix_to_word[predict_label.data[0][0].item()]\n",
    "print('real word is {}, predict word is {}'.format(target_word, predict_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asked,\n",
      "tensor([[-1.1568, -0.9825,  0.3531,  0.8147, -2.9700,  0.0928,  1.0171,\n",
      "          0.6419,  1.0057, -0.7808]])\n",
      "own\n",
      "tensor([[ 0.6147,  0.1798, -2.4165, -3.5395,  0.6888, -1.2381, -0.4775,\n",
      "         -0.6608,  0.0082, -0.4247]])\n",
      "by\n",
      "tensor([[ 1.6343,  1.3534, -1.5896, -0.4293, -0.3689,  1.0900,  0.8851,\n",
      "         -0.2824,  1.9333,  0.0354]])\n",
      "Where\n",
      "tensor([[-1.3096,  0.1470, -0.1365,  0.8540,  1.0137,  0.1830,  0.1384,\n",
      "          0.1679, -1.3315,  1.1201]])\n",
      "How\n",
      "tensor([[ 0.0886, -1.0286, -0.4590, -1.5353, -0.3357,  0.4436,  0.8429,\n",
      "          0.6780,  0.3049, -0.1948]])\n",
      "totter'd\n",
      "tensor([[ 1.3638,  0.5667,  0.9565,  1.4773, -0.7453,  1.6480, -0.4604,\n",
      "          0.3771, -1.0529,  1.7641]])\n",
      "Thy\n",
      "tensor([[ 0.0542,  1.5121,  0.0833,  0.1231, -1.2143, -0.8155, -0.0919,\n",
      "         -1.7810, -0.0789, -1.0564]])\n",
      "thy\n",
      "tensor([[ 1.5832,  2.1953,  0.9612,  0.4272, -1.9099,  0.2299, -0.7458,\n",
      "         -1.6041, -0.2862, -0.6104]])\n",
      "made\n",
      "tensor([[ 0.2354, -1.6590, -1.1956,  0.0889,  0.0549,  1.7187, -0.8777,\n",
      "          0.8435, -0.7315,  0.3684]])\n",
      "mine\n",
      "tensor([[-0.2413,  0.7793,  0.2150,  2.1992, -0.7569, -1.0960, -1.0647,\n",
      "          0.3258, -2.1589,  0.8216]])\n",
      "And\n",
      "tensor([[ 0.6529,  0.3989, -0.3327,  1.1932,  0.2611,  0.9450, -3.1380,\n",
      "         -0.5657, -2.4441,  0.6338]])\n",
      "besiege\n",
      "tensor([[ 2.9966,  2.4958,  1.4384, -1.0752,  0.8888, -0.2019, -0.7711,\n",
      "          1.5693,  0.0098,  0.3126]])\n",
      "and\n",
      "tensor([[ 0.0259,  0.1192, -2.1696,  1.0440, -0.8000, -0.4717,  0.2314,\n",
      "          1.7186,  0.5318, -1.5175]])\n",
      "sum\n",
      "tensor([[-1.8027, -0.1279, -0.8463,  0.0569, -1.3287,  0.9666, -0.1555,\n",
      "         -0.9368,  1.6486,  0.9603]])\n",
      "child\n",
      "tensor([[ 0.7395, -1.1513, -0.0805,  0.1356,  0.4182, -0.2853,  0.2768,\n",
      "         -0.2707,  1.6479,  0.3638]])\n",
      "worth\n",
      "tensor([[ 0.2540, -1.0246,  0.1035,  1.1061,  1.4764, -1.4028,  0.6573,\n",
      "          1.3707,  0.1183, -0.7370]])\n",
      "treasure\n",
      "tensor([[ 0.0227, -1.3542,  0.3848, -0.2160,  0.1542,  2.0216, -0.0030,\n",
      "          0.5073,  1.0152,  0.6988]])\n",
      "see\n",
      "tensor([[ 1.3288,  0.2357,  1.4680,  0.8103,  0.7627,  1.3918,  1.1451,\n",
      "          0.3226, -0.3841,  0.6929]])\n",
      "when\n",
      "tensor([[ 1.0730, -0.1768,  3.0478, -0.3771, -0.0847,  1.0271,  2.2919,\n",
      "         -0.0471, -0.2490,  0.7696]])\n",
      "all\n",
      "tensor([[ 0.8637, -0.6702, -1.0906,  2.3115, -1.5124, -1.4596, -0.9499,\n",
      "          0.1717, -1.0726, -1.5244]])\n",
      "Shall\n",
      "tensor([[-0.2991,  1.2339,  1.6029, -1.2545, -0.9787, -1.2693,  0.8380,\n",
      "         -1.3517,  0.5904, -0.2714]])\n",
      "trenches\n",
      "tensor([[ 0.2586,  1.7606, -0.0835, -1.5375,  0.3825,  0.8099, -0.3453,\n",
      "         -0.3749, -0.9367,  0.8166]])\n",
      "his\n",
      "tensor([[ 0.1774,  0.2178,  0.4341,  0.6047, -0.1937,  0.7299,  0.6210,\n",
      "          0.4990,  0.6342,  2.4643]])\n",
      "small\n",
      "tensor([[-0.2049,  1.1379,  1.3127, -0.0391,  1.1065,  0.7315, -0.2204,\n",
      "          1.1836, -0.2536, -1.4583]])\n",
      "When\n",
      "tensor([[-2.0042,  0.7522, -1.9898, -0.0268, -0.8095,  0.1302,  0.2846,\n",
      "         -0.5341,  0.7727, -0.8821]])\n",
      "art\n",
      "tensor([[ 0.0267, -0.1988,  1.0926,  0.3348,  0.5496, -0.0781,  1.8455,\n",
      "          0.0539,  0.4574,  1.7528]])\n",
      "cold.\n",
      "tensor([[ 0.7794,  0.0335, -0.1250,  0.6071, -0.6004,  0.0100,  0.3114,\n",
      "          0.9093,  0.3022,  1.2416]])\n",
      "answer\n",
      "tensor([[-0.8082, -0.7427,  1.5986,  1.5068,  1.8532,  1.5651,  0.2979,\n",
      "         -1.8525,  0.2221, -0.2229]])\n",
      "weed\n",
      "tensor([[-1.3422, -0.4666, -0.1965, -0.2713,  0.9130,  0.3611,  0.2018,\n",
      "         -0.4061,  0.9265, -0.9586]])\n",
      "shall\n",
      "tensor([[-2.2576,  0.6558,  0.9498,  1.3056,  0.1954, -1.0261,  0.9247,\n",
      "         -0.6629, -0.0679, -0.8700]])\n",
      "If\n",
      "tensor([[ 0.5181, -0.6131,  1.1709, -0.2234, -2.2004,  0.1069,  0.8976,\n",
      "          0.9307, -0.6856, -1.3050]])\n",
      "gazed\n",
      "tensor([[ 1.5649, -2.3556, -1.3309,  0.1253,  0.2120, -1.4268,  0.9121,\n",
      "          0.0657, -0.8690,  1.3393]])\n",
      "a\n",
      "tensor([[ 0.1804,  1.1791,  0.5170,  1.5380,  0.4304,  0.1505,  0.3440,\n",
      "         -3.2666,  1.8679,  0.9041]])\n",
      "fair\n",
      "tensor([[-0.0466,  0.2367,  1.2180, -1.4924, -1.4546, -0.8840,  0.2902,\n",
      "         -0.1865,  1.6429, -0.0516]])\n",
      "more\n",
      "tensor([[ 0.8984, -1.3018,  0.0328,  0.3222, -1.5468, -0.1034,  1.0969,\n",
      "         -1.0798,  0.5877,  1.2841]])\n",
      "feel'st\n",
      "tensor([[ 1.6783, -0.5363, -1.7443,  2.8849,  0.1863, -0.7070,  0.0060,\n",
      "          0.6587,  0.4014,  1.7180]])\n",
      "make\n",
      "tensor([[ 1.3393, -0.3496,  0.7298, -0.2614, -0.6454,  1.3310,  0.2596,\n",
      "          0.1038, -0.8827, -0.3435]])\n",
      "brow,\n",
      "tensor([[-0.1993, -3.3507, -0.3810, -0.3946,  0.3264, -0.2989, -0.0958,\n",
      "          0.4747, -1.0933, -1.2134]])\n",
      "beauty\n",
      "tensor([[-1.3198,  3.1389, -0.3997, -1.9055, -0.0504, -0.4981, -0.3648,\n",
      "          1.1313,  0.1356, -1.8899]])\n",
      "my\n",
      "tensor([[ 0.7484, -1.6098,  2.0544,  0.3267,  1.4557,  1.0451, -0.3803,\n",
      "          0.3703, -0.5085,  1.7660]])\n",
      "deserv'd\n",
      "tensor([[ 0.3363,  1.1999,  0.4959,  0.5467,  0.8653,  2.4590, -0.4921,\n",
      "         -1.3193,  0.0719, -1.7319]])\n",
      "the\n",
      "tensor([[ 1.6467, -0.3031,  1.0213,  0.8829, -1.0308,  0.4253,  1.4321,\n",
      "         -0.7724,  1.4588,  0.6969]])\n",
      "This\n",
      "tensor([[-0.7282, -0.9234, -0.5886,  0.0911,  0.8336, -1.1519, -2.2116,\n",
      "         -0.0627,  0.5338, -1.9456]])\n",
      "so\n",
      "tensor([[-0.2504, -2.4339, -0.1316, -0.1077, -0.9982, -1.4069, -1.3859,\n",
      "          0.9598, -0.2997,  1.4954]])\n",
      "count,\n",
      "tensor([[ 0.3314,  0.7002,  0.0370,  1.4526, -0.6120, -0.6481,  0.7920,\n",
      "          1.0187,  0.1232, -0.1726]])\n",
      "in\n",
      "tensor([[ 1.2885, -1.6637,  0.3633,  0.5670, -0.1388,  0.5766, -2.0041,\n",
      "         -0.8275,  1.7698,  0.7815]])\n",
      "old\n",
      "tensor([[-0.2297, -0.1942, -0.9139,  2.0862, -0.9866,  0.7783, -0.9077,\n",
      "         -0.3129,  2.9550, -1.7341]])\n",
      "forty\n",
      "tensor([[-0.8796,  0.1041,  0.2122, -0.0076, -0.0850, -1.6420,  1.0291,\n",
      "          3.0444,  0.9033,  2.6650]])\n",
      "be\n",
      "tensor([[ 1.2795, -0.8272, -1.4313, -0.2106, -1.9739, -2.4575, -1.3831,\n",
      "         -0.9906, -0.0460,  0.7301]])\n",
      "youth's\n",
      "tensor([[-0.2642,  0.7755, -1.6943,  0.0277, -0.2030, -2.0232, -0.6448,\n",
      "         -2.0333,  1.1877, -2.4624]])\n",
      "deep\n",
      "tensor([[-0.2130, -0.9596, -0.0557,  1.4157,  1.0798,  0.0991, -2.7841,\n",
      "          0.1616, -0.5564, -0.1032]])\n",
      "days;\n",
      "tensor([[-0.7068,  1.0756,  0.1008,  2.8158, -0.2814,  0.4227, -0.7326,\n",
      "          1.3363, -0.0498,  0.9061]])\n",
      "praise.\n",
      "tensor([[-1.3112, -0.7396, -0.1914,  1.9276, -1.8812, -0.3619, -0.6538,\n",
      "          0.2660, -2.1191, -0.7066]])\n",
      "thou\n",
      "tensor([[ 0.3142,  0.0926, -0.2064,  1.0222, -0.2782,  0.5335,  1.6092,\n",
      "          1.9869, -0.6842, -0.3315]])\n",
      "praise\n",
      "tensor([[ 1.4596,  0.8240, -0.6477, -0.0003,  1.3264, -0.4394, -0.3281,\n",
      "         -1.5947, -0.6901, -0.1140]])\n",
      "thine\n",
      "tensor([[-0.1731,  0.1893,  2.1033,  0.2610,  2.0792, -0.4955,  0.2016,\n",
      "         -0.4739, -0.2787,  0.7287]])\n",
      "Will\n",
      "tensor([[-0.7005, -0.5516,  1.0985, -1.3848,  1.6935,  0.8130,  0.3703,\n",
      "          0.1037,  1.1150, -1.6932]])\n",
      "couldst\n",
      "tensor([[-2.7505, -1.5002, -0.4432, -0.1900,  0.3252,  0.6181, -0.1361,\n",
      "         -0.1879,  0.4636,  2.6431]])\n",
      "were\n",
      "tensor([[ 1.6186, -1.7710,  1.7107,  1.1512,  0.3241,  1.2974, -0.2167,\n",
      "          0.3211,  0.4271,  0.1163]])\n",
      "eyes,\n",
      "tensor([[-0.5351,  3.1593,  0.9516,  0.6161, -1.1041,  1.3985,  0.6576,\n",
      "         -0.2040,  1.3104, -1.2295]])\n",
      "thine!\n",
      "tensor([[ 0.0962, -0.1502,  0.7579, -0.1658, -0.0330,  0.0908,  0.1402,\n",
      "          1.5068, -2.2069, -0.5235]])\n",
      "blood\n",
      "tensor([[ 0.1309, -0.3384,  0.7760,  0.1508, -2.4233, -1.0799,  0.2030,\n",
      "          0.3002, -1.5714, -0.1654]])\n",
      "lies,\n",
      "tensor([[ 1.0405, -0.7164, -0.9869, -1.1093, -0.2275, -0.2574, -0.9713,\n",
      "          0.6595, -2.2551, -1.0719]])\n",
      "it\n",
      "tensor([[-1.3352, -1.0605, -0.3062, -0.0146, -0.4342, -1.3602, -0.8322,\n",
      "          0.8338, -0.0273, -1.0181]])\n",
      "new\n",
      "tensor([[-2.2060, -0.0680,  1.2730,  0.6343,  0.6125,  0.3708,  1.3993,\n",
      "          0.1816,  0.1249, -0.7946]])\n",
      "winters\n",
      "tensor([[-0.1179,  0.8479,  0.3823,  2.1372, -0.9881,  0.0761, -2.4880,\n",
      "         -0.4495,  0.8891, -2.5711]])\n",
      "excuse,'\n",
      "tensor([[ 1.0296,  1.7832,  1.8515, -3.6146, -0.0575,  1.5784,  0.7332,\n",
      "          1.0259, -0.0516, -0.1757]])\n",
      "sunken\n",
      "tensor([[-1.9470, -0.1527, -0.7936, -1.1896, -0.7351, -0.7723, -0.9975,\n",
      "         -0.3726,  0.6012, -0.4516]])\n",
      "all-eating\n",
      "tensor([[ 0.3309, -2.0917, -0.7459, -2.5915,  0.2964, -0.1763, -1.3075,\n",
      "          2.4895, -0.0373, -1.5971]])\n",
      "succession\n",
      "tensor([[ 0.5007,  0.4776,  1.9339,  0.8393, -1.2553, -0.3095, -0.2200,\n",
      "          1.9209, -0.1591,  0.1519]])\n",
      "livery\n",
      "tensor([[-1.8983,  0.1092, -0.0748, -0.2202, -0.3597, -0.4430,  1.3020,\n",
      "          0.8845, -0.5429, -1.8105]])\n",
      "thriftless\n",
      "tensor([[-0.2934,  2.6839,  2.1093,  0.4415,  0.9426,  0.9023, -0.1596,\n",
      "          0.4916, -1.5980,  1.2331]])\n",
      "warm\n",
      "tensor([[-0.2616, -1.1039, -1.6795, -0.3379,  0.3457,  0.8710, -0.1863,\n",
      "          0.4343, -0.8674, -1.0213]])\n",
      "where\n",
      "tensor([[ 0.3319, -0.7161,  0.2419, -0.9569, -0.4411, -0.3920, -0.6937,\n",
      "          0.3811,  0.4691,  1.1222]])\n",
      "lusty\n",
      "tensor([[-0.0212, -2.2748, -2.0488, -0.0770,  0.5806, -0.8121,  0.5882,\n",
      "          0.1893, -0.1716, -1.8162]])\n",
      "being\n",
      "tensor([[-0.3899,  1.1182,  0.0630, -0.7956,  1.0329,  0.5464, -1.5000,\n",
      "          0.3996, -0.2133, -2.1291]])\n",
      "of\n",
      "tensor([[-0.8713, -1.4758,  0.5510, -0.1983,  0.5036,  0.9908, -0.9912,\n",
      "          0.0462,  0.9229, -0.2175]])\n",
      "now,\n",
      "tensor([[ 0.4214, -1.4190, -1.0128, -1.2366, -0.1300,  0.9690, -0.8453,\n",
      "         -2.3943, -0.9339, -0.4583]])\n",
      "within\n",
      "tensor([[-0.9951,  1.2839,  0.0265,  1.0044,  0.0040, -0.1208, -0.4459,\n",
      "         -0.8683,  0.0708,  0.4663]])\n",
      "Then\n",
      "tensor([[-0.1551,  0.1123,  1.0765, -0.2129,  0.1705,  0.6645,  1.2247,\n",
      "         -1.4110, -0.5233,  0.3532]])\n",
      "old,\n",
      "tensor([[ 0.5371, -0.6477, -0.0667, -1.0531, -0.5798, -1.4161,  0.4704,\n",
      "         -1.3511, -2.8122, -0.0832]])\n",
      "'This\n",
      "tensor([[-0.8076, -1.2543,  0.0820, -1.1996, -0.1746, -1.4697,  0.7438,\n",
      "         -0.2165, -0.7117, -0.5779]])\n",
      "Were\n",
      "tensor([[-0.5614,  0.9214,  1.4305,  0.8882,  0.9663,  0.3520, -0.3193,\n",
      "          0.7206, -0.5810,  1.2296]])\n",
      "field,\n",
      "tensor([[-0.2414, -0.2559, -0.8001, -0.0056, -0.0944,  0.8260,  0.0719,\n",
      "          0.2919, -0.3981,  0.4687]])\n",
      "an\n",
      "tensor([[ 1.7176, -2.2554, -0.4032,  0.1657,  0.8204, -0.0651, -0.9228,\n",
      "         -1.2221, -2.7852,  0.0474]])\n",
      "dig\n",
      "tensor([[-1.2649, -0.5046,  0.3193, -0.8822, -1.2211,  0.2384,  0.5219,\n",
      "         -0.2414,  1.5014, -0.3218]])\n",
      "Proving\n",
      "tensor([[ 0.4517, -0.4834,  0.3157,  0.6334,  1.1518,  0.3715,  0.4381,\n",
      "         -0.9150,  0.9037, -1.5090]])\n",
      "beauty's\n",
      "tensor([[ 0.2487, -0.1237,  0.1942, -0.6587,  1.8754,  2.0045,  0.0653,\n",
      "          0.2606,  0.1092,  1.4299]])\n",
      "use,\n",
      "tensor([[ 0.0865,  1.1133,  0.8095,  1.3840,  0.5411, -0.0580,  0.3005,\n",
      "         -0.1351,  0.4523,  1.1948]])\n",
      "proud\n",
      "tensor([[ 0.9912,  1.1615,  0.9203, -0.4647,  3.5069,  0.4596,  0.4939,\n",
      "          1.9376, -0.1463, -0.0558]])\n",
      "on\n",
      "tensor([[-1.2174, -1.4364, -0.6750,  0.4721,  0.1822,  0.2593,  2.2658,\n",
      "          0.1667, -0.7085,  1.9855]])\n",
      "held:\n",
      "tensor([[ 0.4491, -0.3180,  2.8358, -0.6921,  0.7450, -1.4298,  1.0368,\n",
      "          0.3031,  0.1746,  0.3835]])\n",
      "To\n",
      "tensor([[-0.9327, -0.5292,  0.4029,  0.7066, -0.5173,  0.6124,  1.0095,\n",
      "         -1.4089, -0.3677,  0.3162]])\n",
      "say,\n",
      "tensor([[ 1.5160,  2.4749, -0.9295,  0.0210,  1.5055, -0.3601, -0.7424,\n",
      "          0.0290,  1.5481,  0.0124]])\n",
      "shame,\n",
      "tensor([[-1.1744,  1.2186, -0.6799, -0.1485, -0.2585,  0.2421,  0.4130,\n",
      "         -0.0712,  0.8109,  1.9003]])\n",
      "to\n",
      "tensor([[ 0.7390, -1.5459, -0.0097,  0.8635, -0.0381, -1.1332,  0.3418,\n",
      "         -1.4745, -1.7685,  0.9172]])\n",
      "much\n",
      "tensor([[-1.2087, -0.8739,  0.3506,  0.1047, -1.0570,  0.6823, -1.3016,\n",
      "          2.1798, -0.8904,  1.8217]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(vocab)):\n",
    "    lookup_tensor = torch.LongTensor([i])\n",
    "    print(ix_to_word[i])\n",
    "    print(embedd(autograd.Variable(lookup_tensor)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
